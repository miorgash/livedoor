{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f0de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cuda',\n",
      " 'DIR_BIN': '/tmp/work/livedoor/bin',\n",
      " 'DIR_DATA': '/tmp/work/livedoor/data',\n",
      " 'DIR_LOG': '/tmp/work/livedoor/log',\n",
      " 'DIR_MECAB_DIC': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
      " 'DIR_MODEL': '/tmp/work/livedoor/model',\n",
      " 'ROOT': '/tmp/work/livedoor',\n",
      " 'SAMPLE_SENT': 'ワンマンライブに行きたい。',\n",
      " 'SEED': 123,\n",
      " 'TOKENIZER': 'mecab'}\n"
     ]
    }
   ],
   "source": [
    "# primitive\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text\n",
    "import MeCab\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# **\n",
    "# handmade libs\n",
    "# *\n",
    "src = '../../src'\n",
    "if src not in sys.path: sys.path.append(src)\n",
    "\n",
    "# constants\n",
    "from const import *\n",
    "constants = {k: v for k, v in locals().items() if k.isupper()}\n",
    "pprint(constants)\n",
    "\n",
    "# modules\n",
    "from my_tokenizer import get_tokenizer\n",
    "from livedoor_dataset import LivedoorDataset\n",
    "from myutils import get_n_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd3178",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4045ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_79517_row0_col0{\n",
       "            background-color:  #084a91;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_79517_row0_col1,#T_79517_row0_col5{\n",
       "            background-color:  #09529d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_79517_row0_col2{\n",
       "            background-color:  #08509b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_79517_row0_col3{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }#T_79517_row0_col4{\n",
       "            background-color:  #105ba4;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_79517_row0_col6{\n",
       "            background-color:  #2d7dbb;\n",
       "            color:  #000000;\n",
       "        }#T_79517_row0_col7{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_79517_row0_col8{\n",
       "            background-color:  #084990;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_79517_\" ><thead>    <tr>        <th class=\"index_name level0\" >0</th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_79517_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_79517_row0_col0\" class=\"data row0 col0\" >698</td>\n",
       "                        <td id=\"T_79517_row0_col1\" class=\"data row0 col1\" >687</td>\n",
       "                        <td id=\"T_79517_row0_col2\" class=\"data row0 col2\" >690</td>\n",
       "                        <td id=\"T_79517_row0_col3\" class=\"data row0 col3\" >392</td>\n",
       "                        <td id=\"T_79517_row0_col4\" class=\"data row0 col4\" >675</td>\n",
       "                        <td id=\"T_79517_row0_col5\" class=\"data row0 col5\" >688</td>\n",
       "                        <td id=\"T_79517_row0_col6\" class=\"data row0 col6\" >632</td>\n",
       "                        <td id=\"T_79517_row0_col7\" class=\"data row0 col7\" >732</td>\n",
       "                        <td id=\"T_79517_row0_col8\" class=\"data row0 col8\" >699</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe2902bffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_4d0df_row0_col0{\n",
       "            background-color:  #2f7fbc;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col1{\n",
       "            background-color:  #0e59a2;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_4d0df_row0_col2{\n",
       "            background-color:  #2c7cba;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col3{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col4{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_4d0df_row0_col5{\n",
       "            background-color:  #7cb7da;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col6{\n",
       "            background-color:  #c6dbef;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col7{\n",
       "            background-color:  #3c8cc3;\n",
       "            color:  #000000;\n",
       "        }#T_4d0df_row0_col8{\n",
       "            background-color:  #3282be;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_4d0df_\" ><thead>    <tr>        <th class=\"index_name level0\" >0</th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4d0df_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_4d0df_row0_col0\" class=\"data row0 col0\" >172</td>\n",
       "                        <td id=\"T_4d0df_row0_col1\" class=\"data row0 col1\" >183</td>\n",
       "                        <td id=\"T_4d0df_row0_col2\" class=\"data row0 col2\" >173</td>\n",
       "                        <td id=\"T_4d0df_row0_col3\" class=\"data row0 col3\" >119</td>\n",
       "                        <td id=\"T_4d0df_row0_col4\" class=\"data row0 col4\" >195</td>\n",
       "                        <td id=\"T_4d0df_row0_col5\" class=\"data row0 col5\" >154</td>\n",
       "                        <td id=\"T_4d0df_row0_col6\" class=\"data row0 col6\" >138</td>\n",
       "                        <td id=\"T_4d0df_row0_col7\" class=\"data row0 col7\" >168</td>\n",
       "                        <td id=\"T_4d0df_row0_col8\" class=\"data row0 col8\" >171</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe27fca75d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "76023"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセット読み込み\n",
    "file = os.path.join(DIR_BIN, 'train_subset.pkl')\n",
    "with open(file, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "file = os.path.join(DIR_BIN, 'test_subset.pkl')\n",
    "with open(file, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# vocab 読み込み\n",
    "TOKENIZER = 'sudachi'\n",
    "DICT = 'core'\n",
    "EMBEDDING = 'chive_mc90'\n",
    "file_vocab = os.path.join(DIR_BIN, f'vocab.{TOKENIZER}.{DICT}.{EMBEDDING}.pkl')\n",
    "with open(file_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "display(get_n_label(train_dataset).style.background_gradient('Blues', axis=1))\n",
    "display(get_n_label(test_dataset).style.background_gradient('Blues', axis=1))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8388002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = vocab.stoi['<pad>']\n",
    "UNK = vocab.stoi['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b3a3c",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7e731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指定した文の分散表現\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class RandomEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(RandomEmbedding, self).__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab.stoi['<pad>'])\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        return self.embedding(texts)\n",
    "\n",
    "random_emb = RandomEmbedding(len(vocab), 100)\n",
    "emb = random_emb(torch.tensor([[2, 3, 4, 5, 6],\n",
    "                               [2, 3, 4, 1, 1]])).detach().numpy()[1]\n",
    "print('指定した文の分散表現')\n",
    "display(sns.heatmap(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a55cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37546 14928 38079 67033 13380 62515 38481 46086 55949 15705 54819 63046\n",
      " 54058 48404 35041 38746 68067 54777 22069 49639 31887 29473 38211  7888\n",
      " 60441 18973 18635 39936 18068  2570 46617 29676 37000 56132 21313 70919\n",
      " 10885 67769 43640  3862 18589 23114 66827 22113 66567 47107 33684   944\n",
      " 68605  6672 15077 67447 15834 48827 52914 56309 15157 15315 21739  1332\n",
      " 60562 52319 26585 16239 26539  8359 25538 72886 38030 37490 68623 74379\n",
      " 34160 63230  9741 58854 57749 56390 50326 29240 25242 57646 75927 61136\n",
      " 39755 12670 70577  7415 16278 18758 22295 46963 38690 37762 45037 45494\n",
      " 26948 39266 70110  5886  3856 37781 59978 18311 73298 71659 37782  9468\n",
      " 69386 72174 61246 71673 62035 41304 44309 39339 42991 40622 61322 13965\n",
      " 13751 51347 28178 58307  1995 44781 17680 56729 30726 22341 47659 46649\n",
      " 52037 35410 53355 30665 37509 63253 75892 44858 75376 75952  2568 43511\n",
      " 26852 20276 14556 66891  1143 28920  5738 22348 11242 73858 21930 66054\n",
      "  2578 49634   266 59912 12156 75179 66995 60274 55086 63717 39012 24449\n",
      " 28181 65808 37772 40768 50636 15280 53670 61010 25087 59096 53518 64057\n",
      "  5807 10662 11787 26927 28424 46304 46556  1489  9995 45154 72642 71821\n",
      " 36973 34651 70929 26851 43155 45231 63839 59314 57231 41542 75560 14140\n",
      " 54801 60930 48998 32119  1928 62439 47392 38636 22800 19349 63221 16099\n",
      " 74585  7924 47722 72093 75226 57548 70122  1795 64043 18940 27441 54831\n",
      "  7424 47147 20038 71585 40484 10454  5354 65527 50170 22176 19691  2001\n",
      " 22897  9727 67097 34560 30468 71807 31810 12022 13699 25706 13908  6904\n",
      " 10496 44238  5520 72926 28880 75593 24538 54072 73861 60490 13960 38900\n",
      " 20539 26155 48394 62734 63645 75701 13259 71160 73858 25169 72297  5091\n",
      "   486 36055 18567 14503 14901 49294 24622 14496 59618 51086 65977 61542\n",
      " 66989 29109 23819 74918 66751 17938 63511 30061 46892 33609 72192 33810]\n",
      "output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, h_dim, class_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        torch.manual_seed(SEED)\n",
    "        self.embedding = RandomEmbedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, h_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(h_dim, class_dim)\n",
    "        \n",
    "    def forward(self, texts):\n",
    "        emb = self.embedding(texts)\n",
    "        o, (h_n, c_n) = self.lstm(emb)\n",
    "        out = self.linear(h_n)\n",
    "        return out\n",
    "\n",
    "lstm = LSTMClassifier(len(vocab), 300, 100, 9)\n",
    "input_ = torch.tensor(np.random.randint(0, 76023, [20, 300]))\n",
    "print(input_.detach().numpy()[0])\n",
    "output = lstm(input_).detach().squeeze().numpy()\n",
    "print('output')\n",
    "sns.heatmap(output, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971f5332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_e7ea3_row0_col0{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e7ea3_row1_col0{\n",
       "            background-color:  #2070b4;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e7ea3_row2_col0{\n",
       "            background-color:  #6aaed6;\n",
       "            color:  #000000;\n",
       "        }#T_e7ea3_row3_col0,#T_e7ea3_row4_col0,#T_e7ea3_row5_col0{\n",
       "            background-color:  #c6dbef;\n",
       "            color:  #000000;\n",
       "        }#T_e7ea3_row6_col0,#T_e7ea3_row7_col0{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_e7ea3_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >1</th>    </tr>    <tr>        <th class=\"index_name level0\" >0</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row0\" class=\"row_heading level0 row0\" >5</th>\n",
       "                        <td id=\"T_e7ea3_row0_col0\" class=\"data row0 col0\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_e7ea3_row1_col0\" class=\"data row1 col0\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_e7ea3_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "                        <td id=\"T_e7ea3_row3_col0\" class=\"data row3 col0\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row4\" class=\"row_heading level0 row4\" >7</th>\n",
       "                        <td id=\"T_e7ea3_row4_col0\" class=\"data row4 col0\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row5\" class=\"row_heading level0 row5\" >8</th>\n",
       "                        <td id=\"T_e7ea3_row5_col0\" class=\"data row5 col0\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row6\" class=\"row_heading level0 row6\" >3</th>\n",
       "                        <td id=\"T_e7ea3_row6_col0\" class=\"data row6 col0\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7ea3_level0_row7\" class=\"row_heading level0 row7\" >4</th>\n",
       "                        <td id=\"T_e7ea3_row7_col0\" class=\"data row7 col0\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe27de7bf90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初期化時点のモデル output の偏り\n",
    "# osx だと満遍なく出力される；Ubuntu18.04 だと一つのラベルに偏る？ Ubuntu20.04 だと解消されてたりしない？\n",
    "freq = Counter(output.argmax(axis=1)).most_common()\n",
    "pd.DataFrame(freq).set_index(0).style.background_gradient('Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a582f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76023\n",
      "76023\n"
     ]
    }
   ],
   "source": [
    "# 対応付が間違っているのではないか？check①\n",
    "# → OK; 系列長が一致する\n",
    "print(len(vocab))\n",
    "emb = [p for p in next(lstm.embedding.parameters())]\n",
    "print(len(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48aae1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 対応付が間違っているのではないか？check②\n",
    "# OK; pad index に対応するのはそれぞれ <pad> トークンと zero ベクトル\n",
    "print(vocab.itos[PAD])\n",
    "print(emb[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfaf73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('sudachi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5211e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentense2index\n",
    "text_pipeline = lambda text: [vocab.stoi[str(token)] for token in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06cda251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザー高速化すれば、学習実施できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfa5f28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-64c5fdcef7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0miter_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-64c5fdcef7f5>\u001b[0m in \u001b[0;36miter_train\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-64c5fdcef7f5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4bb41897d889>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sentense2index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/work/livedoor/src/my_tokenizer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_of_tokenizer\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sudachi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ja_ginza'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# text to list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/ja/__init__.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/ja/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# convert sudachipy.morpheme.Morpheme to DetailedToken and merge continuous spaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0msudachipy_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mdtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_dtokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msudachipy_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mdtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dtokens_and_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/ja/__init__.py\u001b[0m in \u001b[0;36m_get_dtokens\u001b[0;34m(self, sudachipy_tokens, need_sub_tokens)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreading_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# user_data['reading_forms']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0msub_tokens_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msub_tokens_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# user_data['sub_tokens']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             ) for idx, token in enumerate(sudachipy_tokens) if len(token.surface()) > 0\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;31m# remove empty tokens which can be produced with characters like … that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/ja/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0msub_tokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sub_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msudachipy_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneed_sub_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         dtokens = [\n\u001b[0;32m--> 175\u001b[0;31m             DetailedToken(\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# orth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpart_of_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sudachipy/morphemelist.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mmorpheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMorpheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sudachipy/morpheme.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, list_, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "def iter_train(dataloader, model, loss_fn, optimizer):\n",
    "    current_size = 0\n",
    "    current_loss, current_correct = 0, 0\n",
    "    \n",
    "    for batch, (labels, texts) in enumerate(dataloader):\n",
    "        \n",
    "        # indexing\n",
    "        texts = [torch.tensor(text_pipeline(text)) for text in texts]\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=PAD)\n",
    "            \n",
    "        # send to GPU\n",
    "        labels, texts = labels.to(DEVICE), texts.to(DEVICE)\n",
    "\n",
    "        # pred\n",
    "        pred = model(texts)[0]\n",
    "\n",
    "        # get loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # get gradient\n",
    "        optimizer.zero_grad() # バッチごとに勾配をリセット\n",
    "        loss.backward()\n",
    "\n",
    "        # back propagate\n",
    "        # 手動；todo: optimizer.step() の動作確認，デバッグ\n",
    "        for layer_num, param in enumerate(model.parameters()):\n",
    "            if layer_num==0:\n",
    "                continue\n",
    "            LR = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            param.data -= LR * param.grad.data # 正負間違えてるとかないよね\n",
    "#         optimizer.step()\n",
    "        \n",
    "        current_size += len(labels)\n",
    "        current_loss += loss.item()\n",
    "        current_correct += (pred.argmax(axis=1).squeeze()==labels).type(torch.int).sum().item()\n",
    "        \n",
    "        mean_loss = current_loss/current_size\n",
    "        mean_correct = current_correct/current_size\n",
    "\n",
    "        # logging\n",
    "        if batch%30==0 and batch!=0:\n",
    "            split = 'train'\n",
    "            print(f'| {split*(1 if batch==0 else 0):5} | batch {batch:3d} '\n",
    "                  f'| correct total: {mean_correct:0.8f} | loss total: {mean_loss:0.8f} |')\n",
    "    \n",
    "    return mean_correct, mean_loss\n",
    "\n",
    "if DEVICE=='cuda':\n",
    "    # 1 epoch sample execution\n",
    "    sample_dataloader = DataLoader(train_dataset,\n",
    "                                   batch_size=64,\n",
    "                                   shuffle=False,)\n",
    "    LR = 1e-4\n",
    "    model = LSTMClassifier(len(vocab), 300, 100, 9).to(DEVICE)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "    iter_train(sample_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbeca285",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| test  | accuracy: 0.1032 | loss avg: 0.03583317 |"
     ]
    }
   ],
   "source": [
    "# validate\n",
    "def iter_test(dataloader, model, loss_fn):\n",
    "    test_size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 全ての勾配計算を無効化\n",
    "        for batch, (labels, texts) in enumerate(dataloader):\n",
    "            \n",
    "            # indexing\n",
    "            texts = [torch.tensor(text_pipeline(text)) for text in texts]\n",
    "            texts = pad_sequence(texts, batch_first=True, padding_value=PAD)\n",
    "            \n",
    "            # send to GPU\n",
    "            labels, texts = labels.to(DEVICE), texts.to(DEVICE)\n",
    "            \n",
    "            # predict\n",
    "            pred = model(texts)[0]\n",
    "\n",
    "            # get loss\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "\n",
    "            # eval\n",
    "            pred_labels = pred.argmax(axis=1).squeeze() # この squeeze は問題ないのか？\n",
    "            correct += (pred_labels==labels).type(torch.int).sum().item()\n",
    "\n",
    "    correct /= test_size # Epoch 終了時点の Accuracy（正答数 / テストサンプルサイズ）\n",
    "    test_loss /= test_size # Epoch 終了時点の 1 サンプルあたり Loss 平均（Loss / テストサンプルサイズ）\n",
    "\n",
    "    split = 'test'\n",
    "    print(f'| {split:5} | accuracy: {correct:0.4f} | loss avg: {test_loss:6.8f} |', end='')\n",
    "    \n",
    "    return correct, test_loss\n",
    "\n",
    "if DEVICE=='cuda':\n",
    "    # sample execution\n",
    "    sample_dataloader = DataLoader(test_dataset,\n",
    "                                   batch_size=64,\n",
    "                                   shuffle=False,)\n",
    "    LR = 1e-2\n",
    "    model = LSTMClassifier(len(vocab), 300, 100, 9).to(DEVICE)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "    iter_test(sample_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcf8da",
   "metadata": {},
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c856810c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Epoch 0 ----------------------------\n",
      "|       | batch  30 | correct total: 0.09979839 | loss total: 0.06870372 |\n",
      "|       | batch  60 | correct total: 0.10604508 | loss total: 0.06860261 |\n",
      "|       | batch  90 | correct total: 0.10405220 | loss total: 0.06854581 |\n",
      "|       | batch 120 | correct total: 0.10847107 | loss total: 0.06850113 |\n",
      "|       | batch 150 | correct total: 0.11196192 | loss total: 0.06844615 |\n",
      "|       | batch 180 | correct total: 0.11861188 | loss total: 0.06836882 |\n",
      "| test  | accuracy: 0.1195 | loss avg: 0.06977707 | 51.59 sec |\n",
      "---------------------------- Epoch 1 ----------------------------\n",
      "|       | batch  30 | correct total: 0.11693548 | loss total: 0.06842511 |\n",
      "|       | batch  60 | correct total: 0.13217213 | loss total: 0.06823190 |\n",
      "|       | batch  90 | correct total: 0.12568681 | loss total: 0.06813983 |\n",
      "|       | batch 120 | correct total: 0.13248967 | loss total: 0.06812349 |\n",
      "|       | batch 150 | correct total: 0.13327815 | loss total: 0.06812066 |\n",
      "|       | batch 180 | correct total: 0.13259669 | loss total: 0.06817159 |\n",
      "| test  | accuracy: 0.1242 | loss avg: 0.06972815 | 102.73 sec |\n",
      "---------------------------- Epoch 2 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13508065 | loss total: 0.06816220 |\n",
      "|       | batch  60 | correct total: 0.13063525 | loss total: 0.06809447 |\n",
      "|       | batch  90 | correct total: 0.12637363 | loss total: 0.06806280 |\n",
      "|       | batch 120 | correct total: 0.13094008 | loss total: 0.06805111 |\n",
      "|       | batch 150 | correct total: 0.12996689 | loss total: 0.06803954 |\n",
      "|       | batch 180 | correct total: 0.13225138 | loss total: 0.06802604 |\n",
      "| test  | accuracy: 0.1270 | loss avg: 0.06967855 | 154.16 sec |\n",
      "---------------------------- Epoch 3 ----------------------------\n",
      "|       | batch  30 | correct total: 0.12500000 | loss total: 0.06815660 |\n",
      "|       | batch  60 | correct total: 0.13370902 | loss total: 0.06812672 |\n",
      "|       | batch  90 | correct total: 0.12431319 | loss total: 0.06808618 |\n",
      "|       | batch 120 | correct total: 0.12525826 | loss total: 0.06805025 |\n",
      "|       | batch 150 | correct total: 0.12479305 | loss total: 0.06795612 |\n",
      "|       | batch 180 | correct total: 0.12672652 | loss total: 0.06794985 |\n",
      "| test  | accuracy: 0.1297 | loss avg: 0.06971907 | 205.58 sec |\n",
      "---------------------------- Epoch 4 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15221774 | loss total: 0.06781839 |\n",
      "|       | batch  60 | correct total: 0.14139344 | loss total: 0.06787909 |\n",
      "|       | batch  90 | correct total: 0.14320055 | loss total: 0.06789745 |\n",
      "|       | batch 120 | correct total: 0.13920455 | loss total: 0.06794023 |\n",
      "|       | batch 150 | correct total: 0.13721026 | loss total: 0.06790896 |\n",
      "|       | batch 180 | correct total: 0.13950276 | loss total: 0.06781861 |\n",
      "| test  | accuracy: 0.1310 | loss avg: 0.06944164 | 256.64 sec |\n",
      "---------------------------- Epoch 5 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13306452 | loss total: 0.06783425 |\n",
      "|       | batch  60 | correct total: 0.13012295 | loss total: 0.06778257 |\n",
      "|       | batch  90 | correct total: 0.13255495 | loss total: 0.06772841 |\n",
      "|       | batch 120 | correct total: 0.13145661 | loss total: 0.06765934 |\n",
      "|       | batch 150 | correct total: 0.13245033 | loss total: 0.06766685 |\n",
      "|       | batch 180 | correct total: 0.13518646 | loss total: 0.06767590 |\n",
      "| test  | accuracy: 0.1276 | loss avg: 0.06970564 | 308.37 sec |\n",
      "---------------------------- Epoch 6 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15120968 | loss total: 0.06761268 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06748101 |\n",
      "|       | batch  90 | correct total: 0.13907967 | loss total: 0.06761154 |\n",
      "|       | batch 120 | correct total: 0.13894628 | loss total: 0.06765430 |\n",
      "|       | batch 150 | correct total: 0.14176325 | loss total: 0.06759680 |\n",
      "|       | batch 180 | correct total: 0.14209254 | loss total: 0.06760011 |\n",
      "| test  | accuracy: 0.1263 | loss avg: 0.06959155 | 359.31 sec |\n",
      "---------------------------- Epoch 7 ----------------------------\n",
      "|       | batch  30 | correct total: 0.10887097 | loss total: 0.06756033 |\n",
      "|       | batch  60 | correct total: 0.11782787 | loss total: 0.06766183 |\n",
      "|       | batch  90 | correct total: 0.12122253 | loss total: 0.06763566 |\n",
      "|       | batch 120 | correct total: 0.12474174 | loss total: 0.06763246 |\n",
      "|       | batch 150 | correct total: 0.12748344 | loss total: 0.06756010 |\n",
      "|       | batch 180 | correct total: 0.13104282 | loss total: 0.06753032 |\n",
      "| test  | accuracy: 0.1358 | loss avg: 0.06850803 | 410.33 sec |\n",
      "---------------------------- Epoch 8 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14516129 | loss total: 0.06751901 |\n",
      "|       | batch  60 | correct total: 0.14190574 | loss total: 0.06739864 |\n",
      "|       | batch  90 | correct total: 0.13530220 | loss total: 0.06749236 |\n",
      "|       | batch 120 | correct total: 0.14127066 | loss total: 0.06751350 |\n",
      "|       | batch 150 | correct total: 0.14010762 | loss total: 0.06752519 |\n",
      "|       | batch 180 | correct total: 0.14036602 | loss total: 0.06753012 |\n",
      "| test  | accuracy: 0.1337 | loss avg: 0.06964783 | 461.12 sec |\n",
      "---------------------------- Epoch 9 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13004032 | loss total: 0.06724726 |\n",
      "|       | batch  60 | correct total: 0.13780738 | loss total: 0.06726633 |\n",
      "|       | batch  90 | correct total: 0.13839286 | loss total: 0.06737765 |\n",
      "|       | batch 120 | correct total: 0.14075413 | loss total: 0.06743681 |\n",
      "|       | batch 150 | correct total: 0.14279801 | loss total: 0.06734849 |\n",
      "|       | batch 180 | correct total: 0.14071133 | loss total: 0.06732394 |\n",
      "| test  | accuracy: 0.1208 | loss avg: 0.06840627 | 512.20 sec |\n",
      "---------------------------- Epoch 10 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15020161 | loss total: 0.06721418 |\n",
      "|       | batch  60 | correct total: 0.15163934 | loss total: 0.06744569 |\n",
      "|       | batch  90 | correct total: 0.14869505 | loss total: 0.06739765 |\n",
      "|       | batch 120 | correct total: 0.14824380 | loss total: 0.06734851 |\n",
      "|       | batch 150 | correct total: 0.14466060 | loss total: 0.06731749 |\n",
      "|       | batch 180 | correct total: 0.14399171 | loss total: 0.06735090 |\n",
      "| test  | accuracy: 0.1399 | loss avg: 0.06799485 | 563.56 sec |\n",
      "---------------------------- Epoch 11 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13306452 | loss total: 0.06757011 |\n",
      "|       | batch  60 | correct total: 0.14651639 | loss total: 0.06731595 |\n",
      "|       | batch  90 | correct total: 0.14285714 | loss total: 0.06731570 |\n",
      "|       | batch 120 | correct total: 0.14333678 | loss total: 0.06718308 |\n",
      "|       | batch 150 | correct total: 0.14031457 | loss total: 0.06723360 |\n",
      "|       | batch 180 | correct total: 0.13933011 | loss total: 0.06727597 |\n",
      "| test  | accuracy: 0.1283 | loss avg: 0.06772710 | 614.89 sec |\n",
      "---------------------------- Epoch 12 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13608871 | loss total: 0.06676772 |\n",
      "|       | batch  60 | correct total: 0.13883197 | loss total: 0.06703445 |\n",
      "|       | batch  90 | correct total: 0.14114011 | loss total: 0.06703617 |\n",
      "|       | batch 120 | correct total: 0.14023760 | loss total: 0.06712697 |\n",
      "|       | batch 150 | correct total: 0.14052152 | loss total: 0.06714361 |\n",
      "|       | batch 180 | correct total: 0.13967541 | loss total: 0.06722398 |\n",
      "| test  | accuracy: 0.1270 | loss avg: 0.07013722 | 666.21 sec |\n",
      "---------------------------- Epoch 13 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15826613 | loss total: 0.06724568 |\n",
      "|       | batch  60 | correct total: 0.15317623 | loss total: 0.06727531 |\n",
      "|       | batch  90 | correct total: 0.15281593 | loss total: 0.06728309 |\n",
      "|       | batch 120 | correct total: 0.14695248 | loss total: 0.06725208 |\n",
      "|       | batch 150 | correct total: 0.14486755 | loss total: 0.06724966 |\n",
      "|       | batch 180 | correct total: 0.14520028 | loss total: 0.06721563 |\n",
      "| test  | accuracy: 0.1310 | loss avg: 0.06771873 | 717.05 sec |\n",
      "---------------------------- Epoch 14 ----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch  30 | correct total: 0.13104839 | loss total: 0.06713116 |\n",
      "|       | batch  60 | correct total: 0.14036885 | loss total: 0.06710721 |\n",
      "|       | batch  90 | correct total: 0.13427198 | loss total: 0.06706669 |\n",
      "|       | batch 120 | correct total: 0.13842975 | loss total: 0.06714698 |\n",
      "|       | batch 150 | correct total: 0.13907285 | loss total: 0.06712833 |\n",
      "|       | batch 180 | correct total: 0.13777624 | loss total: 0.06711923 |\n",
      "| test  | accuracy: 0.1317 | loss avg: 0.06823461 | 767.89 sec |\n",
      "---------------------------- Epoch 15 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15725806 | loss total: 0.06739320 |\n",
      "|       | batch  60 | correct total: 0.14190574 | loss total: 0.06732751 |\n",
      "|       | batch  90 | correct total: 0.13873626 | loss total: 0.06730462 |\n",
      "|       | batch 120 | correct total: 0.14049587 | loss total: 0.06715057 |\n",
      "|       | batch 150 | correct total: 0.13907285 | loss total: 0.06712671 |\n",
      "|       | batch 180 | correct total: 0.14140193 | loss total: 0.06712258 |\n",
      "| test  | accuracy: 0.1337 | loss avg: 0.07037945 | 818.66 sec |\n",
      "---------------------------- Epoch 16 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13911290 | loss total: 0.06691756 |\n",
      "|       | batch  60 | correct total: 0.13780738 | loss total: 0.06704785 |\n",
      "|       | batch  90 | correct total: 0.13667582 | loss total: 0.06711343 |\n",
      "|       | batch 120 | correct total: 0.13946281 | loss total: 0.06709103 |\n",
      "|       | batch 150 | correct total: 0.13990066 | loss total: 0.06708618 |\n",
      "|       | batch 180 | correct total: 0.14174724 | loss total: 0.06706075 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06748136 | 869.97 sec |\n",
      "---------------------------- Epoch 17 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14112903 | loss total: 0.06692873 |\n",
      "|       | batch  60 | correct total: 0.13729508 | loss total: 0.06706900 |\n",
      "|       | batch  90 | correct total: 0.13873626 | loss total: 0.06692189 |\n",
      "|       | batch 120 | correct total: 0.13868802 | loss total: 0.06701914 |\n",
      "|       | batch 150 | correct total: 0.14176325 | loss total: 0.06699251 |\n",
      "|       | batch 180 | correct total: 0.14053867 | loss total: 0.06699304 |\n",
      "| test  | accuracy: 0.1263 | loss avg: 0.06910732 | 921.23 sec |\n",
      "---------------------------- Epoch 18 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16532258 | loss total: 0.06693900 |\n",
      "|       | batch  60 | correct total: 0.16290984 | loss total: 0.06679257 |\n",
      "|       | batch  90 | correct total: 0.15384615 | loss total: 0.06680936 |\n",
      "|       | batch 120 | correct total: 0.15211777 | loss total: 0.06687965 |\n",
      "|       | batch 150 | correct total: 0.14879967 | loss total: 0.06699140 |\n",
      "|       | batch 180 | correct total: 0.14554558 | loss total: 0.06700325 |\n",
      "| test  | accuracy: 0.1168 | loss avg: 0.06884204 | 972.62 sec |\n",
      "---------------------------- Epoch 19 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13608871 | loss total: 0.06701275 |\n",
      "|       | batch  60 | correct total: 0.13370902 | loss total: 0.06696433 |\n",
      "|       | batch  90 | correct total: 0.13427198 | loss total: 0.06704044 |\n",
      "|       | batch 120 | correct total: 0.13868802 | loss total: 0.06703685 |\n",
      "|       | batch 150 | correct total: 0.13886589 | loss total: 0.06698482 |\n",
      "|       | batch 180 | correct total: 0.14036602 | loss total: 0.06700368 |\n",
      "| test  | accuracy: 0.1283 | loss avg: 0.06797343 | 1023.77 sec |\n",
      "---------------------------- Epoch 20 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14717742 | loss total: 0.06707871 |\n",
      "|       | batch  60 | correct total: 0.14344262 | loss total: 0.06694993 |\n",
      "|       | batch  90 | correct total: 0.14457418 | loss total: 0.06694047 |\n",
      "|       | batch 120 | correct total: 0.14514463 | loss total: 0.06694187 |\n",
      "|       | batch 150 | correct total: 0.14528146 | loss total: 0.06695130 |\n",
      "|       | batch 180 | correct total: 0.14278315 | loss total: 0.06694126 |\n",
      "| test  | accuracy: 0.1337 | loss avg: 0.06795698 | 1075.02 sec |\n",
      "---------------------------- Epoch 21 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14112903 | loss total: 0.06708862 |\n",
      "|       | batch  60 | correct total: 0.13268443 | loss total: 0.06717825 |\n",
      "|       | batch  90 | correct total: 0.13461538 | loss total: 0.06707036 |\n",
      "|       | batch 120 | correct total: 0.13868802 | loss total: 0.06699842 |\n",
      "|       | batch 150 | correct total: 0.14114238 | loss total: 0.06693357 |\n",
      "|       | batch 180 | correct total: 0.14278315 | loss total: 0.06690141 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06840630 | 1126.07 sec |\n",
      "---------------------------- Epoch 22 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14818548 | loss total: 0.06688740 |\n",
      "|       | batch  60 | correct total: 0.13268443 | loss total: 0.06682307 |\n",
      "|       | batch  90 | correct total: 0.13701923 | loss total: 0.06673070 |\n",
      "|       | batch 120 | correct total: 0.13997934 | loss total: 0.06685316 |\n",
      "|       | batch 150 | correct total: 0.13762417 | loss total: 0.06687573 |\n",
      "|       | batch 180 | correct total: 0.14174724 | loss total: 0.06684235 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06917169 | 1177.34 sec |\n",
      "---------------------------- Epoch 23 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15423387 | loss total: 0.06661495 |\n",
      "|       | batch  60 | correct total: 0.15881148 | loss total: 0.06682501 |\n",
      "|       | batch  90 | correct total: 0.15418956 | loss total: 0.06692576 |\n",
      "|       | batch 120 | correct total: 0.14901860 | loss total: 0.06688139 |\n",
      "|       | batch 150 | correct total: 0.15045530 | loss total: 0.06688014 |\n",
      "|       | batch 180 | correct total: 0.14951657 | loss total: 0.06683109 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06767978 | 1229.44 sec |\n",
      "---------------------------- Epoch 24 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14818548 | loss total: 0.06660591 |\n",
      "|       | batch  60 | correct total: 0.14344262 | loss total: 0.06688328 |\n",
      "|       | batch  90 | correct total: 0.14800824 | loss total: 0.06685109 |\n",
      "|       | batch 120 | correct total: 0.14514463 | loss total: 0.06683462 |\n",
      "|       | batch 150 | correct total: 0.14383278 | loss total: 0.06684012 |\n",
      "|       | batch 180 | correct total: 0.14640884 | loss total: 0.06681321 |\n",
      "| test  | accuracy: 0.1527 | loss avg: 0.06846009 | 1280.83 sec |\n",
      "---------------------------- Epoch 25 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13104839 | loss total: 0.06704622 |\n",
      "|       | batch  60 | correct total: 0.13114754 | loss total: 0.06698735 |\n",
      "|       | batch  90 | correct total: 0.13633242 | loss total: 0.06686691 |\n",
      "|       | batch 120 | correct total: 0.13688017 | loss total: 0.06688750 |\n",
      "|       | batch 150 | correct total: 0.13969371 | loss total: 0.06682003 |\n",
      "|       | batch 180 | correct total: 0.14157459 | loss total: 0.06679151 |\n",
      "| test  | accuracy: 0.1270 | loss avg: 0.07013979 | 1332.28 sec |\n",
      "---------------------------- Epoch 26 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15725806 | loss total: 0.06662843 |\n",
      "|       | batch  60 | correct total: 0.14856557 | loss total: 0.06656115 |\n",
      "|       | batch  90 | correct total: 0.15109890 | loss total: 0.06660184 |\n",
      "|       | batch 120 | correct total: 0.15005165 | loss total: 0.06664574 |\n",
      "|       | batch 150 | correct total: 0.14610927 | loss total: 0.06671389 |\n",
      "|       | batch 180 | correct total: 0.14623619 | loss total: 0.06671971 |\n",
      "| test  | accuracy: 0.1371 | loss avg: 0.06906077 | 1383.59 sec |\n",
      "---------------------------- Epoch 27 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16028226 | loss total: 0.06687414 |\n",
      "|       | batch  60 | correct total: 0.16393443 | loss total: 0.06666192 |\n",
      "|       | batch  90 | correct total: 0.15384615 | loss total: 0.06681483 |\n",
      "|       | batch 120 | correct total: 0.15030992 | loss total: 0.06679165 |\n",
      "|       | batch 150 | correct total: 0.14610927 | loss total: 0.06683050 |\n",
      "|       | batch 180 | correct total: 0.14640884 | loss total: 0.06673284 |\n",
      "| test  | accuracy: 0.1371 | loss avg: 0.06791433 | 1434.37 sec |\n",
      "---------------------------- Epoch 28 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15524194 | loss total: 0.06654116 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch  60 | correct total: 0.14600410 | loss total: 0.06661692 |\n",
      "|       | batch  90 | correct total: 0.14388736 | loss total: 0.06676784 |\n",
      "|       | batch 120 | correct total: 0.14669421 | loss total: 0.06676499 |\n",
      "|       | batch 150 | correct total: 0.14755795 | loss total: 0.06673174 |\n",
      "|       | batch 180 | correct total: 0.14882597 | loss total: 0.06669942 |\n",
      "| test  | accuracy: 0.1378 | loss avg: 0.06973346 | 1484.89 sec |\n",
      "---------------------------- Epoch 29 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14213710 | loss total: 0.06666962 |\n",
      "|       | batch  60 | correct total: 0.14754098 | loss total: 0.06654772 |\n",
      "|       | batch  90 | correct total: 0.14182692 | loss total: 0.06659339 |\n",
      "|       | batch 120 | correct total: 0.14204545 | loss total: 0.06662123 |\n",
      "|       | batch 150 | correct total: 0.14362583 | loss total: 0.06665968 |\n",
      "|       | batch 180 | correct total: 0.14554558 | loss total: 0.06663151 |\n",
      "| test  | accuracy: 0.1371 | loss avg: 0.07032643 | 1535.74 sec |\n",
      "---------------------------- Epoch 30 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14919355 | loss total: 0.06646580 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06655987 |\n",
      "|       | batch  90 | correct total: 0.15178571 | loss total: 0.06657618 |\n",
      "|       | batch 120 | correct total: 0.15237603 | loss total: 0.06659669 |\n",
      "|       | batch 150 | correct total: 0.15252483 | loss total: 0.06665080 |\n",
      "|       | batch 180 | correct total: 0.14968923 | loss total: 0.06664046 |\n",
      "| test  | accuracy: 0.1466 | loss avg: 0.06743466 | 1587.55 sec |\n",
      "---------------------------- Epoch 31 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06669214 |\n",
      "|       | batch  60 | correct total: 0.13883197 | loss total: 0.06672557 |\n",
      "|       | batch  90 | correct total: 0.14354396 | loss total: 0.06667942 |\n",
      "|       | batch 120 | correct total: 0.14282025 | loss total: 0.06668582 |\n",
      "|       | batch 150 | correct total: 0.14238411 | loss total: 0.06663970 |\n",
      "|       | batch 180 | correct total: 0.14140193 | loss total: 0.06666472 |\n",
      "| test  | accuracy: 0.1365 | loss avg: 0.07077285 | 1638.29 sec |\n",
      "---------------------------- Epoch 32 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15120968 | loss total: 0.06640037 |\n",
      "|       | batch  60 | correct total: 0.14907787 | loss total: 0.06635887 |\n",
      "|       | batch  90 | correct total: 0.14182692 | loss total: 0.06642066 |\n",
      "|       | batch 120 | correct total: 0.13610537 | loss total: 0.06652863 |\n",
      "|       | batch 150 | correct total: 0.13741722 | loss total: 0.06656622 |\n",
      "|       | batch 180 | correct total: 0.14174724 | loss total: 0.06658117 |\n",
      "| test  | accuracy: 0.1331 | loss avg: 0.06850456 | 1689.81 sec |\n",
      "---------------------------- Epoch 33 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14112903 | loss total: 0.06635123 |\n",
      "|       | batch  60 | correct total: 0.13678279 | loss total: 0.06647330 |\n",
      "|       | batch  90 | correct total: 0.14251374 | loss total: 0.06644381 |\n",
      "|       | batch 120 | correct total: 0.14075413 | loss total: 0.06651285 |\n",
      "|       | batch 150 | correct total: 0.14010762 | loss total: 0.06652033 |\n",
      "|       | batch 180 | correct total: 0.14347376 | loss total: 0.06657823 |\n",
      "| test  | accuracy: 0.1195 | loss avg: 0.06858182 | 1741.50 sec |\n",
      "---------------------------- Epoch 34 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13205645 | loss total: 0.06670288 |\n",
      "|       | batch  60 | correct total: 0.14088115 | loss total: 0.06650719 |\n",
      "|       | batch  90 | correct total: 0.13118132 | loss total: 0.06657863 |\n",
      "|       | batch 120 | correct total: 0.13610537 | loss total: 0.06660070 |\n",
      "|       | batch 150 | correct total: 0.13638245 | loss total: 0.06660839 |\n",
      "|       | batch 180 | correct total: 0.13743094 | loss total: 0.06662340 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06923977 | 1792.45 sec |\n",
      "---------------------------- Epoch 35 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15927419 | loss total: 0.06608390 |\n",
      "|       | batch  60 | correct total: 0.15010246 | loss total: 0.06638144 |\n",
      "|       | batch  90 | correct total: 0.15350275 | loss total: 0.06649768 |\n",
      "|       | batch 120 | correct total: 0.15108471 | loss total: 0.06653663 |\n",
      "|       | batch 150 | correct total: 0.14631623 | loss total: 0.06657139 |\n",
      "|       | batch 180 | correct total: 0.14589088 | loss total: 0.06659042 |\n",
      "| test  | accuracy: 0.1385 | loss avg: 0.06720914 | 1843.10 sec |\n",
      "---------------------------- Epoch 36 ----------------------------\n",
      "|       | batch  30 | correct total: 0.12802419 | loss total: 0.06651851 |\n",
      "|       | batch  60 | correct total: 0.13165984 | loss total: 0.06658646 |\n",
      "|       | batch  90 | correct total: 0.13152473 | loss total: 0.06676756 |\n",
      "|       | batch 120 | correct total: 0.13868802 | loss total: 0.06666134 |\n",
      "|       | batch 150 | correct total: 0.14010762 | loss total: 0.06661999 |\n",
      "|       | batch 180 | correct total: 0.14174724 | loss total: 0.06659026 |\n",
      "| test  | accuracy: 0.1521 | loss avg: 0.06722233 | 1893.82 sec |\n",
      "---------------------------- Epoch 37 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16129032 | loss total: 0.06666244 |\n",
      "|       | batch  60 | correct total: 0.14600410 | loss total: 0.06669042 |\n",
      "|       | batch  90 | correct total: 0.14526099 | loss total: 0.06673838 |\n",
      "|       | batch 120 | correct total: 0.14540289 | loss total: 0.06666273 |\n",
      "|       | batch 150 | correct total: 0.14879967 | loss total: 0.06663616 |\n",
      "|       | batch 180 | correct total: 0.14882597 | loss total: 0.06667443 |\n",
      "| test  | accuracy: 0.1324 | loss avg: 0.06938458 | 1944.96 sec |\n",
      "---------------------------- Epoch 38 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13407258 | loss total: 0.06667174 |\n",
      "|       | batch  60 | correct total: 0.15061475 | loss total: 0.06651455 |\n",
      "|       | batch  90 | correct total: 0.14526099 | loss total: 0.06655553 |\n",
      "|       | batch 120 | correct total: 0.14230372 | loss total: 0.06652164 |\n",
      "|       | batch 150 | correct total: 0.14217715 | loss total: 0.06659677 |\n",
      "|       | batch 180 | correct total: 0.14261050 | loss total: 0.06657661 |\n",
      "| test  | accuracy: 0.1283 | loss avg: 0.06966067 | 1995.98 sec |\n",
      "---------------------------- Epoch 39 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06667182 |\n",
      "|       | batch  60 | correct total: 0.13678279 | loss total: 0.06659152 |\n",
      "|       | batch  90 | correct total: 0.13770604 | loss total: 0.06654642 |\n",
      "|       | batch 120 | correct total: 0.13662190 | loss total: 0.06648596 |\n",
      "|       | batch 150 | correct total: 0.13865894 | loss total: 0.06653171 |\n",
      "|       | batch 180 | correct total: 0.13933011 | loss total: 0.06656499 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06869401 | 2047.38 sec |\n",
      "---------------------------- Epoch 40 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15120968 | loss total: 0.06643919 |\n",
      "|       | batch  60 | correct total: 0.15983607 | loss total: 0.06636361 |\n",
      "|       | batch  90 | correct total: 0.15899725 | loss total: 0.06634558 |\n",
      "|       | batch 120 | correct total: 0.15521694 | loss total: 0.06644584 |\n",
      "|       | batch 150 | correct total: 0.15149007 | loss total: 0.06647405 |\n",
      "|       | batch 180 | correct total: 0.14899862 | loss total: 0.06646562 |\n",
      "| test  | accuracy: 0.1521 | loss avg: 0.07010484 | 2098.99 sec |\n",
      "---------------------------- Epoch 41 ----------------------------\n",
      "|       | batch  30 | correct total: 0.12701613 | loss total: 0.06648774 |\n",
      "|       | batch  60 | correct total: 0.14293033 | loss total: 0.06647226 |\n",
      "|       | batch  90 | correct total: 0.14732143 | loss total: 0.06651580 |\n",
      "|       | batch 120 | correct total: 0.14643595 | loss total: 0.06662385 |\n",
      "|       | batch 150 | correct total: 0.14652318 | loss total: 0.06659542 |\n",
      "|       | batch 180 | correct total: 0.14088398 | loss total: 0.06658358 |\n",
      "| test  | accuracy: 0.1534 | loss avg: 0.06722814 | 2149.24 sec |\n",
      "---------------------------- Epoch 42 ----------------------------\n",
      "|       | batch  30 | correct total: 0.11693548 | loss total: 0.06661015 |\n",
      "|       | batch  60 | correct total: 0.12448770 | loss total: 0.06645627 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch  90 | correct total: 0.13358516 | loss total: 0.06643648 |\n",
      "|       | batch 120 | correct total: 0.13403926 | loss total: 0.06643014 |\n",
      "|       | batch 150 | correct total: 0.13410596 | loss total: 0.06645534 |\n",
      "|       | batch 180 | correct total: 0.13535912 | loss total: 0.06645938 |\n",
      "| test  | accuracy: 0.1358 | loss avg: 0.06864580 | 2200.55 sec |\n",
      "---------------------------- Epoch 43 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16028226 | loss total: 0.06646713 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06634947 |\n",
      "|       | batch  90 | correct total: 0.14697802 | loss total: 0.06639752 |\n",
      "|       | batch 120 | correct total: 0.14695248 | loss total: 0.06643059 |\n",
      "|       | batch 150 | correct total: 0.14797185 | loss total: 0.06649540 |\n",
      "|       | batch 180 | correct total: 0.14640884 | loss total: 0.06649175 |\n",
      "| test  | accuracy: 0.1507 | loss avg: 0.06709253 | 2252.35 sec |\n",
      "---------------------------- Epoch 44 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13407258 | loss total: 0.06638266 |\n",
      "|       | batch  60 | correct total: 0.14395492 | loss total: 0.06636883 |\n",
      "|       | batch  90 | correct total: 0.14697802 | loss total: 0.06634423 |\n",
      "|       | batch 120 | correct total: 0.14514463 | loss total: 0.06637473 |\n",
      "|       | batch 150 | correct total: 0.14093543 | loss total: 0.06641495 |\n",
      "|       | batch 180 | correct total: 0.14381906 | loss total: 0.06644800 |\n",
      "| test  | accuracy: 0.1256 | loss avg: 0.06880598 | 2304.24 sec |\n",
      "---------------------------- Epoch 45 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15020161 | loss total: 0.06657790 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06655273 |\n",
      "|       | batch  90 | correct total: 0.14766484 | loss total: 0.06658177 |\n",
      "|       | batch 120 | correct total: 0.14746901 | loss total: 0.06656221 |\n",
      "|       | batch 150 | correct total: 0.14548841 | loss total: 0.06654984 |\n",
      "|       | batch 180 | correct total: 0.14692680 | loss total: 0.06648816 |\n",
      "| test  | accuracy: 0.1337 | loss avg: 0.06783739 | 2355.84 sec |\n",
      "---------------------------- Epoch 46 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15020161 | loss total: 0.06658648 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06645625 |\n",
      "|       | batch  90 | correct total: 0.14972527 | loss total: 0.06635010 |\n",
      "|       | batch 120 | correct total: 0.14824380 | loss total: 0.06647464 |\n",
      "|       | batch 150 | correct total: 0.14486755 | loss total: 0.06655923 |\n",
      "|       | batch 180 | correct total: 0.14468232 | loss total: 0.06651745 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06973357 | 2407.00 sec |\n",
      "---------------------------- Epoch 47 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14213710 | loss total: 0.06647914 |\n",
      "|       | batch  60 | correct total: 0.14907787 | loss total: 0.06640140 |\n",
      "|       | batch  90 | correct total: 0.14594780 | loss total: 0.06653597 |\n",
      "|       | batch 120 | correct total: 0.14746901 | loss total: 0.06655040 |\n",
      "|       | batch 150 | correct total: 0.14590232 | loss total: 0.06658497 |\n",
      "|       | batch 180 | correct total: 0.14295580 | loss total: 0.06652582 |\n",
      "| test  | accuracy: 0.1460 | loss avg: 0.06874417 | 2458.12 sec |\n",
      "---------------------------- Epoch 48 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14415323 | loss total: 0.06623863 |\n",
      "|       | batch  60 | correct total: 0.13934426 | loss total: 0.06638637 |\n",
      "|       | batch  90 | correct total: 0.14114011 | loss total: 0.06645226 |\n",
      "|       | batch 120 | correct total: 0.14566116 | loss total: 0.06641985 |\n",
      "|       | batch 150 | correct total: 0.14507450 | loss total: 0.06648954 |\n",
      "|       | batch 180 | correct total: 0.14744475 | loss total: 0.06647711 |\n",
      "| test  | accuracy: 0.1365 | loss avg: 0.06847733 | 2509.31 sec |\n",
      "---------------------------- Epoch 49 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14717742 | loss total: 0.06663681 |\n",
      "|       | batch  60 | correct total: 0.14702869 | loss total: 0.06650508 |\n",
      "|       | batch  90 | correct total: 0.14835165 | loss total: 0.06644473 |\n",
      "|       | batch 120 | correct total: 0.14566116 | loss total: 0.06644937 |\n",
      "|       | batch 150 | correct total: 0.14548841 | loss total: 0.06645126 |\n",
      "|       | batch 180 | correct total: 0.14381906 | loss total: 0.06645027 |\n",
      "| test  | accuracy: 0.1371 | loss avg: 0.06881978 | 2560.62 sec |\n",
      "---------------------------- Epoch 50 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15322581 | loss total: 0.06599982 |\n",
      "|       | batch  60 | correct total: 0.14600410 | loss total: 0.06628842 |\n",
      "|       | batch  90 | correct total: 0.15418956 | loss total: 0.06629794 |\n",
      "|       | batch 120 | correct total: 0.15547521 | loss total: 0.06632303 |\n",
      "|       | batch 150 | correct total: 0.15128311 | loss total: 0.06637002 |\n",
      "|       | batch 180 | correct total: 0.14433702 | loss total: 0.06647121 |\n",
      "| test  | accuracy: 0.1236 | loss avg: 0.06713074 | 2611.31 sec |\n",
      "---------------------------- Epoch 51 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15826613 | loss total: 0.06645308 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06653632 |\n",
      "|       | batch  90 | correct total: 0.14388736 | loss total: 0.06656325 |\n",
      "|       | batch 120 | correct total: 0.14566116 | loss total: 0.06651422 |\n",
      "|       | batch 150 | correct total: 0.14776490 | loss total: 0.06646939 |\n",
      "|       | batch 180 | correct total: 0.14520028 | loss total: 0.06645486 |\n",
      "| test  | accuracy: 0.1399 | loss avg: 0.07079096 | 2662.58 sec |\n",
      "---------------------------- Epoch 52 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15423387 | loss total: 0.06632116 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06634731 |\n",
      "|       | batch  90 | correct total: 0.14285714 | loss total: 0.06645132 |\n",
      "|       | batch 120 | correct total: 0.13791322 | loss total: 0.06645369 |\n",
      "|       | batch 150 | correct total: 0.14217715 | loss total: 0.06640564 |\n",
      "|       | batch 180 | correct total: 0.14261050 | loss total: 0.06642887 |\n",
      "| test  | accuracy: 0.1399 | loss avg: 0.06893993 | 2713.58 sec |\n",
      "---------------------------- Epoch 53 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15221774 | loss total: 0.06654882 |\n",
      "|       | batch  60 | correct total: 0.14036885 | loss total: 0.06655295 |\n",
      "|       | batch  90 | correct total: 0.14320055 | loss total: 0.06654094 |\n",
      "|       | batch 120 | correct total: 0.14307851 | loss total: 0.06651190 |\n",
      "|       | batch 150 | correct total: 0.14486755 | loss total: 0.06641633 |\n",
      "|       | batch 180 | correct total: 0.14433702 | loss total: 0.06643448 |\n",
      "| test  | accuracy: 0.1358 | loss avg: 0.06886721 | 2765.23 sec |\n",
      "---------------------------- Epoch 54 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13911290 | loss total: 0.06682349 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06649353 |\n",
      "|       | batch  90 | correct total: 0.14491758 | loss total: 0.06646419 |\n",
      "|       | batch 120 | correct total: 0.14901860 | loss total: 0.06638934 |\n",
      "|       | batch 150 | correct total: 0.14983444 | loss total: 0.06646489 |\n",
      "|       | batch 180 | correct total: 0.14917127 | loss total: 0.06646213 |\n",
      "| test  | accuracy: 0.1297 | loss avg: 0.06740534 | 2816.09 sec |\n",
      "---------------------------- Epoch 55 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15625000 | loss total: 0.06663685 |\n",
      "|       | batch  60 | correct total: 0.16290984 | loss total: 0.06635900 |\n",
      "|       | batch  90 | correct total: 0.15693681 | loss total: 0.06642358 |\n",
      "|       | batch 120 | correct total: 0.15056818 | loss total: 0.06648573 |\n",
      "|       | batch 150 | correct total: 0.15024834 | loss total: 0.06648199 |\n",
      "|       | batch 180 | correct total: 0.14951657 | loss total: 0.06644441 |\n",
      "| test  | accuracy: 0.1385 | loss avg: 0.06733795 | 2867.33 sec |\n",
      "---------------------------- Epoch 56 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15423387 | loss total: 0.06627600 |\n",
      "|       | batch  60 | correct total: 0.16239754 | loss total: 0.06634633 |\n",
      "|       | batch  90 | correct total: 0.15762363 | loss total: 0.06631290 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch 120 | correct total: 0.15599174 | loss total: 0.06643444 |\n",
      "|       | batch 150 | correct total: 0.15231788 | loss total: 0.06639110 |\n",
      "|       | batch 180 | correct total: 0.15003453 | loss total: 0.06641311 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.07001772 | 2918.34 sec |\n",
      "---------------------------- Epoch 57 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16129032 | loss total: 0.06660132 |\n",
      "|       | batch  60 | correct total: 0.15573770 | loss total: 0.06647793 |\n",
      "|       | batch  90 | correct total: 0.14938187 | loss total: 0.06643094 |\n",
      "|       | batch 120 | correct total: 0.14953512 | loss total: 0.06640308 |\n",
      "|       | batch 150 | correct total: 0.14921358 | loss total: 0.06643915 |\n",
      "|       | batch 180 | correct total: 0.14692680 | loss total: 0.06646080 |\n",
      "| test  | accuracy: 0.1236 | loss avg: 0.06753525 | 2969.04 sec |\n",
      "---------------------------- Epoch 58 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14818548 | loss total: 0.06617500 |\n",
      "|       | batch  60 | correct total: 0.14600410 | loss total: 0.06636212 |\n",
      "|       | batch  90 | correct total: 0.14423077 | loss total: 0.06637482 |\n",
      "|       | batch 120 | correct total: 0.14540289 | loss total: 0.06639141 |\n",
      "|       | batch 150 | correct total: 0.14362583 | loss total: 0.06638143 |\n",
      "|       | batch 180 | correct total: 0.14520028 | loss total: 0.06638676 |\n",
      "| test  | accuracy: 0.1358 | loss avg: 0.06789306 | 3020.22 sec |\n",
      "---------------------------- Epoch 59 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13205645 | loss total: 0.06637597 |\n",
      "|       | batch  60 | correct total: 0.14344262 | loss total: 0.06639949 |\n",
      "|       | batch  90 | correct total: 0.14697802 | loss total: 0.06640539 |\n",
      "|       | batch 120 | correct total: 0.14695248 | loss total: 0.06640980 |\n",
      "|       | batch 150 | correct total: 0.14403974 | loss total: 0.06645784 |\n",
      "|       | batch 180 | correct total: 0.14606354 | loss total: 0.06642446 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.07028760 | 3071.26 sec |\n",
      "---------------------------- Epoch 60 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16129032 | loss total: 0.06636530 |\n",
      "|       | batch  60 | correct total: 0.15471311 | loss total: 0.06640845 |\n",
      "|       | batch  90 | correct total: 0.15178571 | loss total: 0.06644902 |\n",
      "|       | batch 120 | correct total: 0.14462810 | loss total: 0.06647289 |\n",
      "|       | batch 150 | correct total: 0.14321192 | loss total: 0.06643557 |\n",
      "|       | batch 180 | correct total: 0.14347376 | loss total: 0.06642277 |\n",
      "| test  | accuracy: 0.1195 | loss avg: 0.06741583 | 3122.55 sec |\n",
      "---------------------------- Epoch 61 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06648289 |\n",
      "|       | batch  60 | correct total: 0.14651639 | loss total: 0.06640683 |\n",
      "|       | batch  90 | correct total: 0.14972527 | loss total: 0.06640378 |\n",
      "|       | batch 120 | correct total: 0.15237603 | loss total: 0.06638551 |\n",
      "|       | batch 150 | correct total: 0.15149007 | loss total: 0.06640821 |\n",
      "|       | batch 180 | correct total: 0.15141575 | loss total: 0.06640469 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06995701 | 3173.76 sec |\n",
      "---------------------------- Epoch 62 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13709677 | loss total: 0.06668200 |\n",
      "|       | batch  60 | correct total: 0.13780738 | loss total: 0.06640816 |\n",
      "|       | batch  90 | correct total: 0.14251374 | loss total: 0.06633203 |\n",
      "|       | batch 120 | correct total: 0.14049587 | loss total: 0.06638794 |\n",
      "|       | batch 150 | correct total: 0.14300497 | loss total: 0.06638261 |\n",
      "|       | batch 180 | correct total: 0.14813536 | loss total: 0.06635131 |\n",
      "| test  | accuracy: 0.1310 | loss avg: 0.06751501 | 3224.60 sec |\n",
      "---------------------------- Epoch 63 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14213710 | loss total: 0.06647075 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06647501 |\n",
      "|       | batch  90 | correct total: 0.14800824 | loss total: 0.06647946 |\n",
      "|       | batch 120 | correct total: 0.14669421 | loss total: 0.06643688 |\n",
      "|       | batch 150 | correct total: 0.14755795 | loss total: 0.06640669 |\n",
      "|       | batch 180 | correct total: 0.14779006 | loss total: 0.06639817 |\n",
      "| test  | accuracy: 0.1392 | loss avg: 0.06991250 | 3275.25 sec |\n",
      "---------------------------- Epoch 64 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15725806 | loss total: 0.06632169 |\n",
      "|       | batch  60 | correct total: 0.15010246 | loss total: 0.06651024 |\n",
      "|       | batch  90 | correct total: 0.15041209 | loss total: 0.06637174 |\n",
      "|       | batch 120 | correct total: 0.14901860 | loss total: 0.06631474 |\n",
      "|       | batch 150 | correct total: 0.15231788 | loss total: 0.06633934 |\n",
      "|       | batch 180 | correct total: 0.15193370 | loss total: 0.06637326 |\n",
      "| test  | accuracy: 0.1392 | loss avg: 0.06906216 | 3326.14 sec |\n",
      "---------------------------- Epoch 65 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13508065 | loss total: 0.06642344 |\n",
      "|       | batch  60 | correct total: 0.13575820 | loss total: 0.06645003 |\n",
      "|       | batch  90 | correct total: 0.13942308 | loss total: 0.06632514 |\n",
      "|       | batch 120 | correct total: 0.13894628 | loss total: 0.06641153 |\n",
      "|       | batch 150 | correct total: 0.14031457 | loss total: 0.06645836 |\n",
      "|       | batch 180 | correct total: 0.14088398 | loss total: 0.06643301 |\n",
      "| test  | accuracy: 0.1378 | loss avg: 0.06942257 | 3376.68 sec |\n",
      "---------------------------- Epoch 66 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15625000 | loss total: 0.06645381 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06641842 |\n",
      "|       | batch  90 | correct total: 0.15796703 | loss total: 0.06634529 |\n",
      "|       | batch 120 | correct total: 0.15779959 | loss total: 0.06636117 |\n",
      "|       | batch 150 | correct total: 0.15562914 | loss total: 0.06637902 |\n",
      "|       | batch 180 | correct total: 0.15348757 | loss total: 0.06640594 |\n",
      "| test  | accuracy: 0.1236 | loss avg: 0.06747719 | 3427.75 sec |\n",
      "---------------------------- Epoch 67 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06640682 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06646500 |\n",
      "|       | batch  90 | correct total: 0.14560440 | loss total: 0.06648956 |\n",
      "|       | batch 120 | correct total: 0.14850207 | loss total: 0.06639698 |\n",
      "|       | batch 150 | correct total: 0.14797185 | loss total: 0.06640420 |\n",
      "|       | batch 180 | correct total: 0.14796271 | loss total: 0.06640056 |\n",
      "| test  | accuracy: 0.1412 | loss avg: 0.06895305 | 3478.96 sec |\n",
      "---------------------------- Epoch 68 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14314516 | loss total: 0.06613610 |\n",
      "|       | batch  60 | correct total: 0.14651639 | loss total: 0.06644472 |\n",
      "|       | batch  90 | correct total: 0.14663462 | loss total: 0.06642338 |\n",
      "|       | batch 120 | correct total: 0.14721074 | loss total: 0.06639816 |\n",
      "|       | batch 150 | correct total: 0.14569536 | loss total: 0.06637984 |\n",
      "|       | batch 180 | correct total: 0.14416436 | loss total: 0.06640383 |\n",
      "| test  | accuracy: 0.1399 | loss avg: 0.06720230 | 3530.35 sec |\n",
      "---------------------------- Epoch 69 ----------------------------\n",
      "|       | batch  30 | correct total: 0.17741935 | loss total: 0.06589860 |\n",
      "|       | batch  60 | correct total: 0.17059426 | loss total: 0.06607348 |\n",
      "|       | batch  90 | correct total: 0.16140110 | loss total: 0.06623803 |\n",
      "|       | batch 120 | correct total: 0.16012397 | loss total: 0.06626863 |\n",
      "|       | batch 150 | correct total: 0.15645695 | loss total: 0.06628377 |\n",
      "|       | batch 180 | correct total: 0.15417818 | loss total: 0.06628837 |\n",
      "| test  | accuracy: 0.1270 | loss avg: 0.06972940 | 3581.15 sec |\n",
      "---------------------------- Epoch 70 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15524194 | loss total: 0.06645213 |\n",
      "|       | batch  60 | correct total: 0.15983607 | loss total: 0.06655174 |\n",
      "|       | batch  90 | correct total: 0.15831044 | loss total: 0.06644082 |\n",
      "|       | batch 120 | correct total: 0.15909091 | loss total: 0.06636604 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch 150 | correct total: 0.15521523 | loss total: 0.06638498 |\n",
      "|       | batch 180 | correct total: 0.15314227 | loss total: 0.06640568 |\n",
      "| test  | accuracy: 0.1534 | loss avg: 0.06734338 | 3632.38 sec |\n",
      "---------------------------- Epoch 71 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14717742 | loss total: 0.06636457 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06636694 |\n",
      "|       | batch  90 | correct total: 0.14732143 | loss total: 0.06639100 |\n",
      "|       | batch 120 | correct total: 0.14669421 | loss total: 0.06639099 |\n",
      "|       | batch 150 | correct total: 0.14735099 | loss total: 0.06639849 |\n",
      "|       | batch 180 | correct total: 0.14589088 | loss total: 0.06637039 |\n",
      "| test  | accuracy: 0.1439 | loss avg: 0.06716485 | 3683.60 sec |\n",
      "---------------------------- Epoch 72 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06618183 |\n",
      "|       | batch  60 | correct total: 0.13883197 | loss total: 0.06622936 |\n",
      "|       | batch  90 | correct total: 0.13942308 | loss total: 0.06631322 |\n",
      "|       | batch 120 | correct total: 0.14256198 | loss total: 0.06634533 |\n",
      "|       | batch 150 | correct total: 0.13907285 | loss total: 0.06636436 |\n",
      "|       | batch 180 | correct total: 0.13881215 | loss total: 0.06637510 |\n",
      "| test  | accuracy: 0.1229 | loss avg: 0.06734924 | 3734.70 sec |\n",
      "---------------------------- Epoch 73 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15725806 | loss total: 0.06633718 |\n",
      "|       | batch  60 | correct total: 0.15215164 | loss total: 0.06638793 |\n",
      "|       | batch  90 | correct total: 0.14972527 | loss total: 0.06644401 |\n",
      "|       | batch 120 | correct total: 0.14798554 | loss total: 0.06641805 |\n",
      "|       | batch 150 | correct total: 0.14983444 | loss total: 0.06629864 |\n",
      "|       | batch 180 | correct total: 0.14658149 | loss total: 0.06636539 |\n",
      "| test  | accuracy: 0.1195 | loss avg: 0.06928069 | 3786.21 sec |\n",
      "---------------------------- Epoch 74 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15221774 | loss total: 0.06653606 |\n",
      "|       | batch  60 | correct total: 0.15881148 | loss total: 0.06629301 |\n",
      "|       | batch  90 | correct total: 0.14800824 | loss total: 0.06630249 |\n",
      "|       | batch 120 | correct total: 0.14359504 | loss total: 0.06629927 |\n",
      "|       | batch 150 | correct total: 0.14279801 | loss total: 0.06633127 |\n",
      "|       | batch 180 | correct total: 0.14105663 | loss total: 0.06636812 |\n",
      "| test  | accuracy: 0.1371 | loss avg: 0.06836104 | 3837.29 sec |\n",
      "---------------------------- Epoch 75 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13608871 | loss total: 0.06620978 |\n",
      "|       | batch  60 | correct total: 0.14241803 | loss total: 0.06610603 |\n",
      "|       | batch  90 | correct total: 0.14732143 | loss total: 0.06618547 |\n",
      "|       | batch 120 | correct total: 0.14643595 | loss total: 0.06625307 |\n",
      "|       | batch 150 | correct total: 0.14424669 | loss total: 0.06629381 |\n",
      "|       | batch 180 | correct total: 0.14261050 | loss total: 0.06632475 |\n",
      "| test  | accuracy: 0.1399 | loss avg: 0.06926048 | 3888.22 sec |\n",
      "---------------------------- Epoch 76 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14919355 | loss total: 0.06645303 |\n",
      "|       | batch  60 | correct total: 0.14446721 | loss total: 0.06631223 |\n",
      "|       | batch  90 | correct total: 0.14251374 | loss total: 0.06628338 |\n",
      "|       | batch 120 | correct total: 0.14178719 | loss total: 0.06639529 |\n",
      "|       | batch 150 | correct total: 0.14424669 | loss total: 0.06638938 |\n",
      "|       | batch 180 | correct total: 0.14364641 | loss total: 0.06638639 |\n",
      "| test  | accuracy: 0.1365 | loss avg: 0.06733859 | 3939.07 sec |\n",
      "---------------------------- Epoch 77 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15725806 | loss total: 0.06642801 |\n",
      "|       | batch  60 | correct total: 0.15061475 | loss total: 0.06637532 |\n",
      "|       | batch  90 | correct total: 0.14869505 | loss total: 0.06637278 |\n",
      "|       | batch 120 | correct total: 0.15211777 | loss total: 0.06636691 |\n",
      "|       | batch 150 | correct total: 0.15293874 | loss total: 0.06636449 |\n",
      "|       | batch 180 | correct total: 0.15245166 | loss total: 0.06635372 |\n",
      "| test  | accuracy: 0.1215 | loss avg: 0.06737213 | 3990.26 sec |\n",
      "---------------------------- Epoch 78 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13810484 | loss total: 0.06629951 |\n",
      "|       | batch  60 | correct total: 0.13524590 | loss total: 0.06630277 |\n",
      "|       | batch  90 | correct total: 0.13839286 | loss total: 0.06628021 |\n",
      "|       | batch 120 | correct total: 0.14462810 | loss total: 0.06634271 |\n",
      "|       | batch 150 | correct total: 0.14197020 | loss total: 0.06637522 |\n",
      "|       | batch 180 | correct total: 0.14261050 | loss total: 0.06633029 |\n",
      "| test  | accuracy: 0.1365 | loss avg: 0.06746439 | 4041.22 sec |\n",
      "---------------------------- Epoch 79 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16935484 | loss total: 0.06611988 |\n",
      "|       | batch  60 | correct total: 0.16393443 | loss total: 0.06621062 |\n",
      "|       | batch  90 | correct total: 0.15899725 | loss total: 0.06622986 |\n",
      "|       | batch 120 | correct total: 0.15599174 | loss total: 0.06630191 |\n",
      "|       | batch 150 | correct total: 0.15521523 | loss total: 0.06628107 |\n",
      "|       | batch 180 | correct total: 0.15314227 | loss total: 0.06629966 |\n",
      "| test  | accuracy: 0.1256 | loss avg: 0.06722871 | 4092.43 sec |\n",
      "---------------------------- Epoch 80 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13306452 | loss total: 0.06633880 |\n",
      "|       | batch  60 | correct total: 0.13473361 | loss total: 0.06652701 |\n",
      "|       | batch  90 | correct total: 0.13289835 | loss total: 0.06640518 |\n",
      "|       | batch 120 | correct total: 0.13507231 | loss total: 0.06643569 |\n",
      "|       | batch 150 | correct total: 0.14031457 | loss total: 0.06638125 |\n",
      "|       | batch 180 | correct total: 0.14122928 | loss total: 0.06636630 |\n",
      "| test  | accuracy: 0.1500 | loss avg: 0.06731715 | 4143.86 sec |\n",
      "---------------------------- Epoch 81 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14415323 | loss total: 0.06641888 |\n",
      "|       | batch  60 | correct total: 0.14651639 | loss total: 0.06633775 |\n",
      "|       | batch  90 | correct total: 0.14423077 | loss total: 0.06618816 |\n",
      "|       | batch 120 | correct total: 0.14462810 | loss total: 0.06627499 |\n",
      "|       | batch 150 | correct total: 0.14486755 | loss total: 0.06630225 |\n",
      "|       | batch 180 | correct total: 0.14450967 | loss total: 0.06628925 |\n",
      "| test  | accuracy: 0.1541 | loss avg: 0.06703205 | 4195.15 sec |\n",
      "---------------------------- Epoch 82 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13608871 | loss total: 0.06619467 |\n",
      "|       | batch  60 | correct total: 0.14395492 | loss total: 0.06629579 |\n",
      "|       | batch  90 | correct total: 0.14114011 | loss total: 0.06630989 |\n",
      "|       | batch 120 | correct total: 0.14204545 | loss total: 0.06636710 |\n",
      "|       | batch 150 | correct total: 0.14383278 | loss total: 0.06634719 |\n",
      "|       | batch 180 | correct total: 0.14036602 | loss total: 0.06633852 |\n",
      "| test  | accuracy: 0.1392 | loss avg: 0.06823385 | 4246.62 sec |\n",
      "---------------------------- Epoch 83 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14213710 | loss total: 0.06633474 |\n",
      "|       | batch  60 | correct total: 0.15163934 | loss total: 0.06637557 |\n",
      "|       | batch  90 | correct total: 0.15350275 | loss total: 0.06628276 |\n",
      "|       | batch 120 | correct total: 0.14721074 | loss total: 0.06627213 |\n",
      "|       | batch 150 | correct total: 0.14962748 | loss total: 0.06630930 |\n",
      "|       | batch 180 | correct total: 0.14917127 | loss total: 0.06628062 |\n",
      "| test  | accuracy: 0.1378 | loss avg: 0.06963821 | 4297.98 sec |\n",
      "---------------------------- Epoch 84 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13608871 | loss total: 0.06641602 |\n",
      "|       | batch  60 | correct total: 0.13934426 | loss total: 0.06634781 |\n",
      "|       | batch  90 | correct total: 0.13804945 | loss total: 0.06635985 |\n",
      "|       | batch 120 | correct total: 0.14152893 | loss total: 0.06632653 |\n",
      "|       | batch 150 | correct total: 0.14300497 | loss total: 0.06633227 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | batch 180 | correct total: 0.14589088 | loss total: 0.06632972 |\n",
      "| test  | accuracy: 0.1351 | loss avg: 0.06923423 | 4349.28 sec |\n",
      "---------------------------- Epoch 85 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13508065 | loss total: 0.06644250 |\n",
      "|       | batch  60 | correct total: 0.14754098 | loss total: 0.06630428 |\n",
      "|       | batch  90 | correct total: 0.14491758 | loss total: 0.06633429 |\n",
      "|       | batch 120 | correct total: 0.14746901 | loss total: 0.06631573 |\n",
      "|       | batch 150 | correct total: 0.14921358 | loss total: 0.06630693 |\n",
      "|       | batch 180 | correct total: 0.14727210 | loss total: 0.06633680 |\n",
      "| test  | accuracy: 0.1453 | loss avg: 0.06733219 | 4400.56 sec |\n",
      "---------------------------- Epoch 86 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15826613 | loss total: 0.06648421 |\n",
      "|       | batch  60 | correct total: 0.15471311 | loss total: 0.06642677 |\n",
      "|       | batch  90 | correct total: 0.15109890 | loss total: 0.06635113 |\n",
      "|       | batch 120 | correct total: 0.14746901 | loss total: 0.06632675 |\n",
      "|       | batch 150 | correct total: 0.14942053 | loss total: 0.06633349 |\n",
      "|       | batch 180 | correct total: 0.14951657 | loss total: 0.06632457 |\n",
      "| test  | accuracy: 0.1358 | loss avg: 0.06774092 | 4452.48 sec |\n",
      "---------------------------- Epoch 87 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16129032 | loss total: 0.06627365 |\n",
      "|       | batch  60 | correct total: 0.14856557 | loss total: 0.06637971 |\n",
      "|       | batch  90 | correct total: 0.14800824 | loss total: 0.06635929 |\n",
      "|       | batch 120 | correct total: 0.14901860 | loss total: 0.06641496 |\n",
      "|       | batch 150 | correct total: 0.14673013 | loss total: 0.06641401 |\n",
      "|       | batch 180 | correct total: 0.14727210 | loss total: 0.06636737 |\n",
      "| test  | accuracy: 0.1256 | loss avg: 0.06802353 | 4503.84 sec |\n",
      "---------------------------- Epoch 88 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14818548 | loss total: 0.06608039 |\n",
      "|       | batch  60 | correct total: 0.14446721 | loss total: 0.06622676 |\n",
      "|       | batch  90 | correct total: 0.14560440 | loss total: 0.06631359 |\n",
      "|       | batch 120 | correct total: 0.14824380 | loss total: 0.06630683 |\n",
      "|       | batch 150 | correct total: 0.14693709 | loss total: 0.06627447 |\n",
      "|       | batch 180 | correct total: 0.14796271 | loss total: 0.06630528 |\n",
      "| test  | accuracy: 0.1222 | loss avg: 0.06934515 | 4555.73 sec |\n",
      "---------------------------- Epoch 89 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15423387 | loss total: 0.06640697 |\n",
      "|       | batch  60 | correct total: 0.14856557 | loss total: 0.06635946 |\n",
      "|       | batch  90 | correct total: 0.15178571 | loss total: 0.06639209 |\n",
      "|       | batch 120 | correct total: 0.14979339 | loss total: 0.06639551 |\n",
      "|       | batch 150 | correct total: 0.14755795 | loss total: 0.06636056 |\n",
      "|       | batch 180 | correct total: 0.14709945 | loss total: 0.06635985 |\n",
      "| test  | accuracy: 0.1507 | loss avg: 0.06751319 | 4607.17 sec |\n",
      "---------------------------- Epoch 90 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14516129 | loss total: 0.06611178 |\n",
      "|       | batch  60 | correct total: 0.15163934 | loss total: 0.06609057 |\n",
      "|       | batch  90 | correct total: 0.14869505 | loss total: 0.06625831 |\n",
      "|       | batch 120 | correct total: 0.14824380 | loss total: 0.06629844 |\n",
      "|       | batch 150 | correct total: 0.14569536 | loss total: 0.06630652 |\n",
      "|       | batch 180 | correct total: 0.14727210 | loss total: 0.06629276 |\n",
      "| test  | accuracy: 0.1385 | loss avg: 0.06781987 | 4658.87 sec |\n",
      "---------------------------- Epoch 91 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15423387 | loss total: 0.06633688 |\n",
      "|       | batch  60 | correct total: 0.14805328 | loss total: 0.06630345 |\n",
      "|       | batch  90 | correct total: 0.14560440 | loss total: 0.06629586 |\n",
      "|       | batch 120 | correct total: 0.14798554 | loss total: 0.06637961 |\n",
      "|       | batch 150 | correct total: 0.14817881 | loss total: 0.06637448 |\n",
      "|       | batch 180 | correct total: 0.14571823 | loss total: 0.06635779 |\n",
      "| test  | accuracy: 0.1385 | loss avg: 0.06914330 | 4710.06 sec |\n",
      "---------------------------- Epoch 92 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13205645 | loss total: 0.06608007 |\n",
      "|       | batch  60 | correct total: 0.14395492 | loss total: 0.06616425 |\n",
      "|       | batch  90 | correct total: 0.14697802 | loss total: 0.06623353 |\n",
      "|       | batch 120 | correct total: 0.14850207 | loss total: 0.06621586 |\n",
      "|       | batch 150 | correct total: 0.14797185 | loss total: 0.06626987 |\n",
      "|       | batch 180 | correct total: 0.14571823 | loss total: 0.06628454 |\n",
      "| test  | accuracy: 0.1466 | loss avg: 0.06718962 | 4761.38 sec |\n",
      "---------------------------- Epoch 93 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14415323 | loss total: 0.06622756 |\n",
      "|       | batch  60 | correct total: 0.13985656 | loss total: 0.06633158 |\n",
      "|       | batch  90 | correct total: 0.14010989 | loss total: 0.06637062 |\n",
      "|       | batch 120 | correct total: 0.14101240 | loss total: 0.06634949 |\n",
      "|       | batch 150 | correct total: 0.14548841 | loss total: 0.06631644 |\n",
      "|       | batch 180 | correct total: 0.14727210 | loss total: 0.06630865 |\n",
      "| test  | accuracy: 0.1419 | loss avg: 0.06950507 | 4812.76 sec |\n",
      "---------------------------- Epoch 94 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13407258 | loss total: 0.06613917 |\n",
      "|       | batch  60 | correct total: 0.14702869 | loss total: 0.06617720 |\n",
      "|       | batch  90 | correct total: 0.13701923 | loss total: 0.06621813 |\n",
      "|       | batch 120 | correct total: 0.14075413 | loss total: 0.06627435 |\n",
      "|       | batch 150 | correct total: 0.14590232 | loss total: 0.06627976 |\n",
      "|       | batch 180 | correct total: 0.14779006 | loss total: 0.06630188 |\n",
      "| test  | accuracy: 0.1385 | loss avg: 0.06764800 | 4864.15 sec |\n",
      "---------------------------- Epoch 95 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15221774 | loss total: 0.06660875 |\n",
      "|       | batch  60 | correct total: 0.14600410 | loss total: 0.06652518 |\n",
      "|       | batch  90 | correct total: 0.15075549 | loss total: 0.06638727 |\n",
      "|       | batch 120 | correct total: 0.15185950 | loss total: 0.06635801 |\n",
      "|       | batch 150 | correct total: 0.15004139 | loss total: 0.06636466 |\n",
      "|       | batch 180 | correct total: 0.15245166 | loss total: 0.06634840 |\n",
      "| test  | accuracy: 0.1426 | loss avg: 0.07026968 | 4915.09 sec |\n",
      "---------------------------- Epoch 96 ----------------------------\n",
      "|       | batch  30 | correct total: 0.14717742 | loss total: 0.06640940 |\n",
      "|       | batch  60 | correct total: 0.15215164 | loss total: 0.06627058 |\n",
      "|       | batch  90 | correct total: 0.15041209 | loss total: 0.06629469 |\n",
      "|       | batch 120 | correct total: 0.14876033 | loss total: 0.06631033 |\n",
      "|       | batch 150 | correct total: 0.14673013 | loss total: 0.06630149 |\n",
      "|       | batch 180 | correct total: 0.14917127 | loss total: 0.06628164 |\n",
      "| test  | accuracy: 0.1534 | loss avg: 0.06716372 | 4966.57 sec |\n",
      "---------------------------- Epoch 97 ----------------------------\n",
      "|       | batch  30 | correct total: 0.15927419 | loss total: 0.06602920 |\n",
      "|       | batch  60 | correct total: 0.14959016 | loss total: 0.06625762 |\n",
      "|       | batch  90 | correct total: 0.15041209 | loss total: 0.06627425 |\n",
      "|       | batch 120 | correct total: 0.14979339 | loss total: 0.06629019 |\n",
      "|       | batch 150 | correct total: 0.14279801 | loss total: 0.06635571 |\n",
      "|       | batch 180 | correct total: 0.14450967 | loss total: 0.06632071 |\n",
      "| test  | accuracy: 0.1263 | loss avg: 0.06724866 | 5017.60 sec |\n",
      "---------------------------- Epoch 98 ----------------------------\n",
      "|       | batch  30 | correct total: 0.13407258 | loss total: 0.06642831 |\n",
      "|       | batch  60 | correct total: 0.13627049 | loss total: 0.06624886 |\n",
      "|       | batch  90 | correct total: 0.14079670 | loss total: 0.06631571 |\n",
      "|       | batch 120 | correct total: 0.13868802 | loss total: 0.06635802 |\n",
      "|       | batch 150 | correct total: 0.13886589 | loss total: 0.06630694 |\n",
      "|       | batch 180 | correct total: 0.14243785 | loss total: 0.06629972 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| test  | accuracy: 0.1392 | loss avg: 0.06714992 | 5068.11 sec |\n",
      "---------------------------- Epoch 99 ----------------------------\n",
      "|       | batch  30 | correct total: 0.16633065 | loss total: 0.06617903 |\n",
      "|       | batch  60 | correct total: 0.15881148 | loss total: 0.06628424 |\n",
      "|       | batch  90 | correct total: 0.15556319 | loss total: 0.06632357 |\n",
      "|       | batch 120 | correct total: 0.15573347 | loss total: 0.06635371 |\n",
      "|       | batch 150 | correct total: 0.15086921 | loss total: 0.06631406 |\n",
      "|       | batch 180 | correct total: 0.15124309 | loss total: 0.06630367 |\n",
      "| test  | accuracy: 0.1405 | loss avg: 0.06837358 | 5119.10 sec |\n",
      "done!\n",
      "/tmp/work/livedoor/model/emb=random&edim=300&model=lstm&hdim=100&batchsize=032&optim=sgd&lr=0.10000&mo=0.9&epochs= 100&shuffle=True.pth : created\n",
      "/tmp/work/livedoor/log/emb=random&edim=300&model=lstm&hdim=100&batchsize=032&optim=sgd&lr=0.10000&mo=0.9&epochs= 100&shuffle=True.csv : created\n"
     ]
    }
   ],
   "source": [
    "# model setting\n",
    "model_name = 'lstm'\n",
    "embed_type = 'random'\n",
    "embed_dim = 300\n",
    "hidden_dim = 100\n",
    "\n",
    "# training setting\n",
    "batchsizes = [32]\n",
    "lrs = [1e-1]\n",
    "mos = [0.9]\n",
    "n_epochss = [100]\n",
    "\n",
    "for (batchsize, lr, mo, n_epochs) in itertools.product(batchsizes, lrs, mos, n_epochss):\n",
    "    # dataloader\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batchsize,\n",
    "                                  shuffle=True,)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                  batch_size=batchsize,\n",
    "                                  shuffle=True,)\n",
    "\n",
    "    # define network architecture\n",
    "    model = LSTMClassifier(len(vocab), embed_dim, hidden_dim, 9).to(DEVICE)\n",
    "    \n",
    "    # modules\n",
    "    loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=mo)\n",
    "    \n",
    "    # train model\n",
    "    result = []\n",
    "    start = time()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f'{\"-\"*28} Epoch {epoch} {\"-\"*28}')\n",
    "        train_acc, train_loss = iter_train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_acc, test_loss = iter_test(test_dataloader, model, loss_fn)\n",
    "        print(f' {time()-start:5.2f} sec |')\n",
    "        result.append((epoch, train_acc, train_loss, test_acc, test_loss))\n",
    "\n",
    "    print('done!')\n",
    "    \n",
    "    # **\n",
    "    # save\n",
    "    # *\n",
    "    filename_base = f'emb={embed_type}&edim={embed_dim:03d}' \\\n",
    "                    f'&model={model_name}&hdim={hidden_dim:03d}' \\\n",
    "                    f'&batchsize={batchsize:03d}' \\\n",
    "                    f'&optim=sgd&lr={lr:.5f}&mo={mo:.1f}' \\\n",
    "                    f'&epochs={n_epochs:4d}' \\\n",
    "                    f'&shuffle=True'\n",
    "    filename_model = os.path.join(DIR_MODEL, filename_base+'.pth')\n",
    "    filename_log = os.path.join(DIR_LOG, filename_base+'.csv')\n",
    "\n",
    "    # save model\n",
    "    if not os.path.isfile(filename_model):\n",
    "        torch.save(model, filename_model)\n",
    "        print(f'{filename_model} : created')\n",
    "    else:\n",
    "        print(f'{filename_model} : exists')\n",
    "\n",
    "    # save log\n",
    "    if not os.path.isfile(filename_log):\n",
    "        result = pd.DataFrame(result).rename(columns={0: 'epoch',\n",
    "                                                      1: 'train_acc',\n",
    "                                                      2: 'train_loss',\n",
    "                                                      3: 'test_acc',\n",
    "                                                      4: 'test_loss'})\n",
    "        result.to_csv(filename_log, index=False)\n",
    "        print(f'{filename_log} : created')\n",
    "    else:\n",
    "        print(f'{filename_log} : exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ee00c",
   "metadata": {},
   "source": [
    "過学習、バッチ shuffle してなかったからでは？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea77ef",
   "metadata": {},
   "source": [
    "そもそもおかしいところ\n",
    "- 特定のラベルに推論結果が偏る\n",
    "- test なのに正答しない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db3e9ed",
   "metadata": {},
   "source": [
    "なやみ\n",
    "- 過学習する\n",
    "- 収束しない\n",
    "- ９値分類で推定結果が１つだけに偏る；１epoch 目からそんな感じ．初期値がかたよってるから？（散らばらせることできる？）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ea349",
   "metadata": {},
   "source": [
    "- embedding の 重みって，分散表現とべつであるか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde176b",
   "metadata": {},
   "source": [
    "- model 生成の順序を修正したから改善した？Embedding.require_grad=False を無効にしたから改善した？（先に進む前に，一度確認すべき）\n",
    "- 32, 64 128 で 1e-0 で試してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becda21d",
   "metadata": {},
   "source": [
    "- train, test ともに，モデルの出力結果（Loss()に入れる直前の生の出力値）を受け取って評価するつくりに変更\n",
    "- → train, test をスタンドアロンに動かせるようになる．\n",
    "- → テストだけ実行して，未学習時点における推論結果の偏り有無を確認する（偏ってたら，パラメータ初期値が好ましくないという仮説の立証に一歩近づく）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba06fb0",
   "metadata": {},
   "source": [
    "embedding の weight 更新 True にして試行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163c340",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bedbcab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mcrosstab\u001b[0;34m(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_colnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     }\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommon_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0moriginal_df_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             )\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def predict_dataloader(dataloader, model):\n",
    "\n",
    "    y = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 全ての勾配計算を無効化\n",
    "        for batch, (labels, texts) in enumerate(dataloader):\n",
    "            \n",
    "            y.append(list(labels.numpy()))\n",
    "            \n",
    "            # indexing\n",
    "            texts = [torch.tensor(text_pipeline(text)) for text in texts]\n",
    "            texts = pad_sequence(texts, batch_first=True, padding_value=PAD)\n",
    "            \n",
    "            # send to GPU\n",
    "            labels, texts = labels.to(DEVICE), texts.to(DEVICE)\n",
    "            \n",
    "            # predict\n",
    "            if batch==0:\n",
    "                y_pred = model(texts)[0].cpu().numpy()\n",
    "            else:\n",
    "                y_pred = np.append(y_pred, model(texts)[0].cpu().numpy(), axis=0)\n",
    "    \n",
    "    return np.array(list(itertools.chain.from_iterable(y))), y_pred#.argmax(axis=1)\n",
    "\n",
    "f = 'emb=random&edim=300&model=lstm&hdim=100&batchsize=128&optim=sgd&lr=0.00003&mo=0.9&epochs=1.pth'\n",
    "model = torch.load(os.path.join(DIR_MODEL, f))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=128,\n",
    "                              shuffle=False,)\n",
    "y, y_pred = predict_dataloader(test_dataloader, model)\n",
    "sns.heatmap(pd.crosstab(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab96bda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10310535, -0.05537033,  0.0144057 , ...,  0.02722622,\n",
       "         0.00404025,  0.07488791],\n",
       "       [ 0.10310535, -0.05537033,  0.01440569, ...,  0.02722622,\n",
       "         0.00404026,  0.07488791],\n",
       "       [ 0.10310535, -0.05537033,  0.0144057 , ...,  0.02722622,\n",
       "         0.00404026,  0.07488791],\n",
       "       ...,\n",
       "       [ 0.10310535, -0.05537033,  0.0144057 , ...,  0.02722622,\n",
       "         0.00404026,  0.07488791],\n",
       "       [ 0.10310535, -0.05537033,  0.0144057 , ...,  0.02722622,\n",
       "         0.00404026,  0.07488791],\n",
       "       [ 0.10310535, -0.05537033,  0.0144057 , ...,  0.02722622,\n",
       "         0.00404026,  0.07488791]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_word_embedding(model, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c856c8",
   "metadata": {},
   "source": [
    "# Primitive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb1324",
   "metadata": {},
   "source": [
    "create feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d404d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5893, 300) (5893,)\n",
      "(1473, 300) (1473,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_table(dataloader):\n",
    "    X, y = [], []\n",
    "    for batch, (labels, texts) in enumerate(dataloader):\n",
    "        texts = [torch.tensor(text_pipeline(text)) for text in texts]\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=PAD)\n",
    "        labels, texts = labels.to(DEVICE), texts.to(DEVICE)\n",
    "        emb = model.embedding(texts).detach().cpu().numpy().mean(axis=1)\n",
    "        X.append(emb)\n",
    "        y.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    X = np.array(list(itertools.chain.from_iterable(X)))\n",
    "    y = np.array(list(itertools.chain.from_iterable(y)))\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "model = LSTMClassifier(len(vocab), 300, 100, 9).to(DEVICE)\n",
    "X_train, y_train = get_embedding_table(train_dataloader)\n",
    "X_test, y_test = get_embedding_table(test_dataloader)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704fb0e",
   "metadata": {},
   "source": [
    "feature overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b6e955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 300)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x311.04 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show(embedding):\n",
    "    w = 18\n",
    "    h = embedding.shape[0] * (w / 300) * 8\n",
    "\n",
    "    fig = plt.figure(figsize=(w, h))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    print(embedding.shape)\n",
    "    sns.heatmap(pd.DataFrame(embedding), cbar=False, xticklabels=False, yticklabels=False, cmap='Blues')\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "show(df.groupby(y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb6d4e",
   "metadata": {},
   "source": [
    "ベンチマーク；SVM で 82 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38e83bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268839103869654"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "345b8459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "sns.heatmap(pd.crosstab(y_test, y_pred), cbar=False, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59c3a0",
   "metadata": {},
   "source": [
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# embed\n",
    "wv = KeyedVectors.load_word2vec_format('/data/chive/chive-1.2-mc5/chive-1.2-mc5.txt')\n",
    "wv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
