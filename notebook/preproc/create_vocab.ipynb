{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f0de66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cpu',\n",
      " 'DIR_BIN': '/tmp/work/livedoor/bin',\n",
      " 'DIR_DATA': '/tmp/work/livedoor/data',\n",
      " 'DIR_LOG': '/tmp/work/livedoor/log',\n",
      " 'DIR_MECAB_DIC': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
      " 'DIR_MODEL': '/tmp/work/livedoor/model',\n",
      " 'ROOT': '/tmp/work/livedoor',\n",
      " 'SAMPLE_SENT': 'ワンマンライブに行きたい。',\n",
      " 'SEED': 123,\n",
      " 'TOKENIZER': 'mecab'}\n"
     ]
    }
   ],
   "source": [
    "# primitive\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text\n",
    "import MeCab\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# **\n",
    "# handmade libs\n",
    "# *\n",
    "src = '../../src'\n",
    "if src not in sys.path: sys.path.append(src)\n",
    "\n",
    "# constants\n",
    "from const import *\n",
    "constants = {k: v for k, v in locals().items() if k.isupper()}\n",
    "pprint(constants)\n",
    "\n",
    "# modules\n",
    "from my_tokenizer import get_tokenizer\n",
    "from livedoor_dataset import LivedoorDataset\n",
    "from sudachi_tokenizer import SudachiTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd3178",
   "metadata": {},
   "source": [
    "# Preprocess for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1840358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(DIR_BIN, 'train_subset.pkl')\n",
    "with open(file, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "# file = os.path.join(DIR_BIN, 'test_subset.pkl')\n",
    "# with open(file, 'rb') as f:\n",
    "#     test_dataset = pickle.load(f)\n",
    "\n",
    "vectors = gensim.models.KeyedVectors.load('/data/chive_v1.2mc90/chive-1.2-mc90_gensim/chive-1.2-mc90.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd29bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding の語彙で freq = 0 で作成し、\n",
    "# 学習用データの語彙で update すればよいのでは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479ddc0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def create_vocab(corpus, embedding_words: set):\n",
    "    '''トークナイズ済みコーパスから Vocab を作成する\n",
    "    \n",
    "    Args:\n",
    "      corpus: text の list\n",
    "    Returns:\n",
    "      vocab: index と word のマッピング\n",
    "    '''\n",
    "    \n",
    "    counter = Counter()\n",
    "    for text in tqdm(corpus):\n",
    "        counter.update(text)\n",
    "    \n",
    "    words = list(counter.keys())\n",
    "    for word in tqdm(words):\n",
    "        if word not in embedding_words:\n",
    "            _ = counter.pop(word)\n",
    "    \n",
    "    return Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f723718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [41:26,  2.37it/s] \n",
      "100%|██████████| 75970/75970 [00:00<00:00, 456993.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: /tmp/work/livedoor/bin/vocab.sudachi.core.chive_mc90.intersection.pkl\n",
      "CPU times: user 29min 40s, sys: 20.9 s, total: 30min 1s\n",
      "Wall time: 41min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ENGINE = 'sudachi'\n",
    "DICT = 'core'\n",
    "DATASET = 'livedoor'\n",
    "EMBEDDING = 'chive_mc90'\n",
    "VERSION = 'intersection'\n",
    "file_vocab = os.path.join(DIR_BIN, f'vocab.{ENGINE}.{DICT}.{DATASET}.{EMBEDDING}.{VERSION}.pkl')\n",
    "\n",
    "if os.path.isfile(file_vocab):\n",
    "    print(f'file exists: {file_vocab}')\n",
    "    pass\n",
    "else:\n",
    "    tokenizer = SudachiTokenizer()\n",
    "    corpus = map(lambda row: row[1], train_dataset) # 学習用データセットからテキストだけ取得\n",
    "    corpus = tokenizer.tokenized_corpus(corpus) # トークン列のジェネレータ\n",
    "    embedding_words = set([vectors.index2word[i] for i in range(len(vectors.vocab))]) # 分散表現に含まれる語彙\n",
    "    vocab = create_vocab(corpus, embedding_words)\n",
    "#     vocab = create_vocab([s for s in corpus][:10], embedding_words) # for test\n",
    "    \n",
    "    print(f'create: {file_vocab}')\n",
    "    with open(file_vocab, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "# Wall time: 27min 36s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
