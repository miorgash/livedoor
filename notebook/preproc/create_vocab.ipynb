{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f0de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cpu',\n",
      " 'DIR_BIN': '/tmp/work/livedoor/bin',\n",
      " 'DIR_DATA': '/tmp/work/livedoor/data',\n",
      " 'DIR_LOG': '/tmp/work/livedoor/log',\n",
      " 'DIR_MECAB_DIC': '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd',\n",
      " 'DIR_MODEL': '/tmp/work/livedoor/model',\n",
      " 'ROOT': '/tmp/work/livedoor',\n",
      " 'SAMPLE_SENT': 'ワンマンライブに行きたい。',\n",
      " 'SEED': 123,\n",
      " 'TOKENIZER': 'mecab'}\n"
     ]
    }
   ],
   "source": [
    "# primitive\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text\n",
    "import MeCab\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# **\n",
    "# handmade libs\n",
    "# *\n",
    "src = '../../src'\n",
    "if src not in sys.path: sys.path.append(src)\n",
    "\n",
    "# constants\n",
    "from const import *\n",
    "constants = {k: v for k, v in locals().items() if k.isupper()}\n",
    "pprint(constants)\n",
    "\n",
    "# modules\n",
    "from my_tokenizer import get_tokenizer\n",
    "from livedoor_dataset import LivedoorDataset\n",
    "from sudachi_tokenizer import SudachiTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd3178",
   "metadata": {},
   "source": [
    "# Preprocess for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1840358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(DIR_BIN, 'train_subset.pkl')\n",
    "with open(file, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "file = os.path.join(DIR_BIN, 'test_subset.pkl')\n",
    "with open(file, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479ddc0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def create_vocab(corpus):\n",
    "    '''トークナイズ済みコーパスから Vocab を作成する\n",
    "    \n",
    "    Args:\n",
    "      corpus: text の list\n",
    "    Returns:\n",
    "      vocab: index と word のマッピング\n",
    "    '''\n",
    "    \n",
    "    counter = Counter()\n",
    "    for text in tqdm(corpus):\n",
    "        counter.update(text)\n",
    "    \n",
    "    return Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f723718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [27:36,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: /tmp/work/livedoor/bin/vocab.sudachi.core.chive_mc90.pkl\n",
      "CPU times: user 28min 32s, sys: 20.2 s, total: 28min 53s\n",
      "Wall time: 27min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ENGINE = 'sudachi'\n",
    "DICT = 'core'\n",
    "EMBEDDING = 'chive_mc90'\n",
    "file_vocab = os.path.join(DIR_BIN, f'vocab.{ENGINE}.{DICT}.{EMBEDDING}.pkl')\n",
    "\n",
    "if os.path.isfile(file_vocab):\n",
    "    print(f'file exists: {file_vocab}')\n",
    "    pass\n",
    "else:\n",
    "    tokenizer = SudachiTokenizer()\n",
    "    corpus = map(lambda row: row[1], train_dataset) # 学習用データセットからテキストだけ取得\n",
    "    corpus = tokenizer.tokenized_corpus(corpus) # トークン列のジェネレータ\n",
    "    vocab = create_vocab(corpus)\n",
    "    \n",
    "    print(f'create: {file_vocab}')\n",
    "    with open(file_vocab, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "# Wall time: 27min 36s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
